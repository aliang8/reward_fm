#!/usr/bin/env python3
"""
VLM evaluation server - drop-in replacement for server.py using Gemini.
Same API, different backend.
"""

import base64
import io
from typing import Any, Dict, List

import numpy as np
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
from pydantic import BaseModel

from vlm_baseline import VLMPreferenceBaseline


# Same payload models as main server.py
class SamplePayload(BaseModel):
    task: str = ""
    prediction_type: str
    chosen_frames_b64: List[str]
    rejected_frames_b64: List[str]
    target_progress_A: List[float] = None
    target_progress_B: List[float] = None


class BatchPayload(BaseModel):
    samples: List[SamplePayload]


def create_app(task_description: str = "", use_temporal_prompts: bool = False) -> FastAPI:
    """Create FastAPI app with VLM backend."""
    
    app = FastAPI(title="VLM Evaluation Server")
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Initialize VLM - fail fast if API key missing
    print("Initializing VLM baseline...")
    print("Frame handling: Using all frames provided by client")
    if use_temporal_prompts:
        print("Prompting: Temporal-aware for >4 total frames (EXPERIMENTAL)")
    else:
        print("Prompting: RL-VLM-F baseline (DEFAULT)")
    
    vlm = VLMPreferenceBaseline(
        vlm_provider="gemini", 
        verbose=True,
        use_temporal_prompts=use_temporal_prompts,
        log_dir="vlm_eval_logs"  # Enable detailed logging
    )
    print("VLM ready!")
    
    # Add shutdown handler to finalize logs
    @app.on_event("shutdown")
    def shutdown_event():
        print("ðŸ”„ Finalizing evaluation logs...")
        vlm.finalize_log()
    
    def decode_b64_image(b64_str: str) -> Image.Image:
        """Decode base64 to PIL Image."""
        img_data = base64.b64decode(b64_str)
        return Image.open(io.BytesIO(img_data))
    
    def compute_metrics(samples: List[SamplePayload]) -> Dict[str, Any]:
        """Compute evaluation metrics using VLM - matching RL-VLM-F capabilities."""
        
        results = []
        for sample in samples:
            if sample.prediction_type != "preference":
                continue
            
            # Decode images from base64
            chosen_images = [decode_b64_image(b64) for b64 in sample.chosen_frames_b64]
            rejected_images = [decode_b64_image(b64) for b64 in sample.rejected_frames_b64]
            
            # Query VLM for preference
            result = vlm.query_preference(
                chosen_images,
                rejected_images,
                task_description or sample.task
            )
            
            results.append(result)
        
        if not results:
            return _empty_metrics()
        
        # RL-VLM-F only gets discrete labels - we can only compute accuracy
        num_samples = len(results)
        total_correct = sum(r["is_correct"] for r in results)
        
        return {
            "eval_loss": 0.0,                               # Placeholder: no probabilities available
            "eval_accuracy": total_correct / num_samples,   # Meaningful: % correct preferences
            "eval_reward_diff": 0.0,                        # Placeholder: no absolute reward scale
            "eval_avg_reward_chosen": 0.0,                  # Placeholder: no absolute reward scale
            "eval_avg_reward_rejected": 0.0,                # Placeholder: no absolute reward scale
            "demo_reward_alignment": []                     # Placeholder: VLM doesn't predict progress
        }
    
    def _empty_metrics():
        return {
            "eval_loss": 0.0,                      # No samples to evaluate
            "eval_accuracy": 0.0,                  # No samples to evaluate
            "eval_reward_diff": 0.0,               # Placeholder
            "eval_avg_reward_chosen": 0.0,         # Placeholder
            "eval_avg_reward_rejected": 0.0,       # Placeholder
            "demo_reward_alignment": []            # Placeholder
        }
    
    @app.post("/evaluate_batch")
    def evaluate_batch(batch: BatchPayload):
        """Evaluate batch using VLM - same API as main server."""
        pref_samples = [s for s in batch.samples if s.prediction_type == "preference"]
        
        if not pref_samples:
            return _empty_metrics()
        
        return compute_metrics(pref_samples)
    
    return app


def main():
    import argparse
    import uvicorn
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", default="0.0.0.0")
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument("--task", default="", help="Task description for VLM queries")
    parser.add_argument("--temporal", action="store_true", 
                       help="Enable temporal-aware prompting for multi-frame trajectories (experimental)")
    args = parser.parse_args()
    
    print(f"\n{'='*50}")
    print(f"VLM Evaluation Server")
    if args.task:
        print(f"Task: {args.task}")
    print(f"Frame handling: Client-driven (uses all frames sent)")
    if args.temporal:
        print(f"Prompting: Temporal-aware (EXPERIMENTAL)")
    else:
        print(f"Prompting: RL-VLM-F baseline (DEFAULT)")
    print(f"Same API as main server.py")
    print(f"{'='*50}\n")
    
    app = create_app(args.task, args.temporal)
    uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main() 
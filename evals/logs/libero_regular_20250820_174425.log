Running LIBERO regular dataset evaluations at Wed Aug 20 17:44:25 PDT 2025
=== Evaluating subset: libero_10 ===
Start time: Wed Aug 20 17:44:25 PDT 2025
Applied config override: evaluation.eval_dataset_path = abraranwar/libero_rfm
Applied config override: evaluation.eval_dataset_subsets = [libero_10]
Setting up evaluation data generator on rank 0...
Loading 2 evaluation datasets with corresponding subsets
  Dataset 1: abraranwar/libero_rfm -> libero_10
  Dataset 2: ykorkmaz/libero_failure_rfm -> libero_10_failure
Found preprocessed evaluation cache at ./processed_datasets/eval_cache, loading...
Loaded 997 trajectory indices from preprocessed evaluation cache
Successfully loaded preprocessed evaluation cache with 997 trajectory indices
No preference dataset provided, will use random sampling for preferences
No similarity dataset provided, will use random sampling for similarities
DataGenerator initialized with 997 total trajectories
  Robot trajectories: 997
  Human trajectories: 0
  Tasks: 10
  Quality labels: 2
  Data sources: 2
Evaluation data generator initialized on rank 0
Evaluating (all prefs):   0%|          | 0/42 [00:00<?, ?it/s]Evaluating (all prefs):   2%|▏         | 1/42 [00:38<26:10, 38.30s/it]Evaluating (all prefs):   5%|▍         | 2/42 [00:47<14:17, 21.44s/it]Evaluating (all prefs):   7%|▋         | 3/42 [00:52<08:49, 13.57s/it]Evaluating (all prefs):  10%|▉         | 4/42 [01:51<20:07, 31.77s/it]Evaluating (all prefs):  12%|█▏        | 5/42 [02:08<16:10, 26.23s/it]Evaluating (all prefs):  14%|█▍        | 6/42 [02:12<11:14, 18.75s/it]Evaluating (all prefs):  17%|█▋        | 7/42 [02:16<08:13, 14.10s/it]Evaluating (all prefs):  19%|█▉        | 8/42 [02:21<06:15, 11.03s/it]Evaluating (all prefs):  21%|██▏       | 9/42 [02:33<06:11, 11.25s/it]Evaluating (all prefs):  24%|██▍       | 10/42 [02:44<06:05, 11.41s/it]Evaluating (all prefs):  26%|██▌       | 11/42 [02:49<04:50,  9.39s/it]Evaluating (all prefs):  29%|██▊       | 12/42 [02:55<04:07,  8.24s/it]Evaluating (all prefs):  31%|███       | 13/42 [03:00<03:32,  7.33s/it]Evaluating (all prefs):  33%|███▎      | 14/42 [03:09<03:40,  7.88s/it]Evaluating (all prefs):  36%|███▌      | 15/42 [03:15<03:14,  7.22s/it]Evaluating (all prefs):  38%|███▊      | 16/42 [03:20<02:49,  6.52s/it]Evaluating (all prefs):  40%|████      | 17/42 [03:25<02:30,  6.03s/it]Evaluating (all prefs):  43%|████▎     | 18/42 [03:28<02:06,  5.28s/it]Evaluating (all prefs):  45%|████▌     | 19/42 [03:32<01:50,  4.82s/it]Evaluating (all prefs):  48%|████▊     | 20/42 [03:38<01:54,  5.21s/it]Evaluating (all prefs):  50%|█████     | 21/42 [03:43<01:45,  5.04s/it]Evaluating (all prefs):  52%|█████▏    | 22/42 [03:47<01:37,  4.86s/it]Evaluating (all prefs):  55%|█████▍    | 23/42 [03:56<01:52,  5.91s/it]Evaluating (all prefs):  57%|█████▋    | 24/42 [05:17<08:35, 28.65s/it]Evaluating (all prefs):  60%|█████▉    | 25/42 [05:28<06:34, 23.21s/it]Evaluating (all prefs):  62%|██████▏   | 26/42 [05:35<04:53, 18.35s/it]Evaluating (all prefs):  64%|██████▍   | 27/42 [05:42<03:42, 14.86s/it]Evaluating (all prefs):  67%|██████▋   | 28/42 [05:47<02:46, 11.91s/it]Evaluating (all prefs):  69%|██████▉   | 29/42 [05:51<02:06,  9.71s/it]Evaluating (all prefs):  71%|███████▏  | 30/42 [05:56<01:40,  8.35s/it]Evaluating (all prefs):  74%|███████▍  | 31/42 [06:01<01:20,  7.31s/it]Evaluating (all prefs):  76%|███████▌  | 32/42 [06:06<01:04,  6.42s/it]Evaluating (all prefs):  79%|███████▊  | 33/42 [06:10<00:52,  5.79s/it]Evaluating (all prefs):  81%|████████  | 34/42 [06:14<00:42,  5.37s/it]Evaluating (all prefs):  83%|████████▎ | 35/42 [06:18<00:34,  4.95s/it]Evaluating (all prefs):  86%|████████▌ | 36/42 [06:22<00:27,  4.66s/it]Evaluating (all prefs):  88%|████████▊ | 37/42 [06:26<00:22,  4.45s/it]Evaluating (all prefs):  90%|█████████ | 38/42 [06:30<00:16,  4.23s/it]Evaluating (all prefs):  93%|█████████▎| 39/42 [06:33<00:12,  4.01s/it]Evaluating (all prefs):  95%|█████████▌| 40/42 [06:37<00:07,  3.88s/it]Evaluating (all prefs):  98%|█████████▊| 41/42 [06:40<00:03,  3.78s/it]Evaluating (all prefs): 100%|██████████| 42/42 [06:43<00:00,  3.29s/it]Evaluating (all prefs): 100%|██████████| 42/42 [06:43<00:00,  9.60s/it]
batch metrics: {'eval_accuracy': 0.6666666666666666, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.25, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.3333333333333333, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.75, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.5, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.5833333333333334, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
batch metrics: {'eval_accuracy': 0.0, 'eval_reward_diff': None, 'eval_avg_reward_chosen': None, 'eval_avg_reward_rejected': None, 'demo_reward_alignment': None}
WARNING: eval_reward_diff is always None, removing
WARNING: eval_avg_reward_chosen is always None, removing
WARNING: eval_avg_reward_rejected is always None, removing
WARNING: demo_reward_alignment is always None, removing

Evaluation summary (averaged across batches):
  eval_accuracy: 0.073413
explanation:    Accuracy of Predicting the Correct Preference
End time: Wed Aug 20 17:51:44 PDT 2025
=== Completed subset: libero_10 ===

Completed all LIBERO regular dataset evaluations at Wed Aug 20 17:51:44 PDT 2025

# FSDP Training Configuration for RFM
# This config is optimized for distributed training with FSDP

# General settings
mode: "train"
debug: false

# Model configuration
model:
  base_model_id: "Qwen/Qwen2.5-VL-3B-Instruct"
  torch_dtype: "bfloat16"
  trust_remote_code: true

# PEFT/LoRA configuration
peft:
  use_peft: true  # Enable PEFT for efficient training
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  bias: "none"
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  train_vision_encoder: false
  train_language_model: true
  train_value_head: true
  train_progress_head: true
  train_preference_head: true
  train_similarity_head: true

# Data configuration
data:
  dataset_path: "aliangdw/rfm"
  dataset_subsets: ["libero_10"]
  base_dir: "libero_dpo_dataset"
  max_frames: 4
  video_frame_sampling: "uniform"
  resized_height: 128
  resized_width: 128
  preference_ratio: 0.5
  similarity_ratio: 0.5
  dataset_preference_ratio: 0.7
  shuffle: true
  seed: 42
  num_proc: 1
  force_reprocess: false
  dataloader_pin_memory: false
  dataloader_num_workers: 0

# Training configuration (optimized for FSDP)
training:
  num_gpus: 4  # Number of GPUs for distributed training
  output_dir: "./rfm_model_output_fsdp"
  max_seq_length: 1024
  beta: 0.1
  resume_from_checkpoint: null
  per_device_train_batch_size: 1  # Small batch size per device for FSDP
  gradient_accumulation_steps: 16  # Effective batch size = 1 * 16 * 4 = 64
  learning_rate: 5e-7
  num_train_epochs: 1000
  save_strategy: "steps"
  logging_steps: 10
  save_steps: 100
  bf16: false  # Use fp16 for FSDP
  fp16: true
  remove_unused_columns: false
  gradient_checkpointing: false  # Disabled for FSDP2 compatibility (causes DTensor mixing issues)
  ddp_find_unused_parameters: false
  ddp_bucket_cap_mb: 25
  max_steps: -1
  # fsdp_strategy: "fsdp2"  # Use FSDP2 for better performance

# Logging configuration
logging:
  print_trainable_parameters: true
  save_model: true
  save_processor: true
  use_wandb: true
  wandb_project: "rfm-model-fsdp"
  wandb_entity: null
  wandb_run_name: "rfm-fsdp-training"

# Evaluation configuration
evaluation:
  model_path: "./rfm_model_output_fsdp"
  eval_subset_size: 10
  eval_dataset_path: "aliangdw/rfm"
  eval_base_dir: "libero_dpo_dataset"
  eval_dataset_subsets: ["libero_90"] 
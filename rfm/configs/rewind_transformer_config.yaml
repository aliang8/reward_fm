# ReWIND Transformer Configuration
trainer_cls: "rewind_transformer"

# =====================================================================================
# Model Configuration
# =====================================================================================
model:
  base_model_id: "rewind_transformer"  # custom transformer model

  # set rewind specific parameters
  rewind:
    video_feature_dim: 768
    text_feature_dim: 384
    hidden_dim: 512
    num_layers: 4
    num_attention_heads: 8
    dropout: 0.1  
    max_len: 16

  train_vision_encoder: false
  train_language_model: false

training:
  output_dir: "./logs"
  gradient_checkpointing: false

  bf16: false
  fp16: true

  per_device_train_batch_size: 1024
  per_device_eval_batch_size: 512
  learning_rate: 1e-4
  warmup_ratio: 0.01
  
  eval_steps: 100
  custom_eval_steps: 100
  max_steps: 10000

data:
  # sample_type_ratio: [0, 1, 0] # only progress samples 
  # progress_strategy_ratio: [1, 1, 1] # default success, rewind, different task
  sample_type_ratio: [0, 1, 1] # preference, progress, similarity 
  preference_strategy_ratio: [6, 1, 1, 0] # rewind, suboptimal_same_task, different_task, video_binned 
  progress_strategy_ratio: [1, 6, 1] # default success, rewind, different task
  
  dataset_type: balanced_mixed
  # dataset_type: default

  # this is based on the subset name
  data_source_weights:
    metaworld_train: 5
    libero256_90: 1
    libero_90_failure: 1

  load_embeddings: true
  progress_pred_type: "absolute" 

# =====================================================================================
# Logging and Output Configuration
# =====================================================================================
logging:
  wandb_project: "rfm"
  wandb_entity: clvr